"arg:-wA=1024 -hA=1024 -wB=1024 -hB=1024" 
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
Adjust Iters to 1477 for meeting time requirement 10 secs.
iterated 1477, average time is 6.742690 msec
Performance= 318.49 GFlop/s, Time= 6.743 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
iterated 50, average time is 6.789179 msec
Performance= 316.31 GFlop/s, Time= 6.789 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
==13804== NVPROF is profiling process 13804, command: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==13804== Warning: Unified Memory Profiling is not supported on the current configuration because a pair of devices without peer-to-peer support is detected on this multi-GPU setup. When peer mappings are not available, system falls back to using zero-copy memory. It can cause kernels, which access unified memory, to run slower. More details can be found at: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-managed-memory
==13804== Profiling application: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==13804== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 98.41%  348.33ms        52  6.6987ms  6.6826ms  6.7292ms  void matrixMulCUDA<int=32>(float*, float*, float*, int, int)
  0.93%  3.2938ms         2  1.6469ms  1.6068ms  1.6871ms  [CUDA memcpy HtoD]
  0.66%  2.3475ms         1  2.3475ms  2.3475ms  2.3475ms  [CUDA memcpy DtoH]

==13804== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 54.74%  346.42ms        51  6.7926ms  6.7609ms  6.8243ms  cudaThreadSynchronize
 31.65%  200.30ms         3  66.768ms  1.1075ms  198.08ms  cudaMalloc
 10.06%  63.680ms         1  63.680ms  63.680ms  63.680ms  cudaDeviceReset
  1.18%  7.4949ms         3  2.4983ms  7.1060us  7.4804ms  cudaDeviceSynchronize
  1.01%  6.3880ms         3  2.1293ms  1.2792ms  3.1053ms  cudaMemcpy
  0.59%  3.7475ms        51  73.479us  49.746us  115.70us  cudaEventSynchronize
  0.20%  1.2385ms       182  6.8050us       0ns  322.92us  cuDeviceGetAttribute
  0.14%  909.36us        52  17.487us  15.918us  54.579us  cudaLaunch
  0.13%  822.10us       102  8.0590us  5.6850us  11.371us  cudaEventRecord
  0.10%  602.64us         1  602.64us  602.64us  602.64us  cudaGetDeviceProperties
  0.08%  514.80us         3  171.60us  142.13us  187.05us  cudaFree
  0.08%  491.78us        51  9.6420us  9.3800us  10.517us  cudaEventElapsedTime
  0.03%  163.74us         2  81.868us  69.929us  93.808us  cuDeviceGetName
  0.01%  55.431us       260     213ns       0ns  1.1370us  cudaSetupArgument
  0.00%  28.426us        52     546ns     284ns  3.1270us  cudaConfigureCall
  0.00%  12.792us         1  12.792us  12.792us  12.792us  cudaSetDevice
  0.00%  11.371us         2  5.6850us  5.4010us  5.9700us  cuDeviceTotalMem
  0.00%  7.1050us         2  3.5520us     852ns  6.2530us  cudaEventCreate
  0.00%  3.1270us         3  1.0420us     284ns  2.5580us  cuDeviceGetCount
  0.00%  2.8440us         6     474ns     284ns  1.1370us  cuDeviceGet
  0.00%  2.2740us         1  2.2740us  2.2740us  2.2740us  cudaGetDevice
