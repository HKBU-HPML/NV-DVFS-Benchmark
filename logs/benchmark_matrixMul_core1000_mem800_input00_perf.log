"arg:-wA=1024 -hA=1024 -wB=1024 -hB=1024" 
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
Adjust Iters to 2328 for meeting time requirement 10 secs.
iterated 2328, average time is 4.306584 msec
Performance= 498.65 GFlop/s, Time= 4.307 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
iterated 50, average time is 4.322403 msec
Performance= 496.83 GFlop/s, Time= 4.322 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
==24432== NVPROF is profiling process 24432, command: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==24432== Warning: Unified Memory Profiling is not supported on the current configuration because a pair of devices without peer-to-peer support is detected on this multi-GPU setup. When peer mappings are not available, system falls back to using zero-copy memory. It can cause kernels, which access unified memory, to run slower. More details can be found at: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-managed-memory
==24432== Profiling application: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==24432== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 97.63%  220.30ms        52  4.2366ms  4.2161ms  4.2576ms  void matrixMulCUDA<int=32>(float*, float*, float*, int, int)
  1.46%  3.2951ms         2  1.6475ms  1.5986ms  1.6965ms  [CUDA memcpy HtoD]
  0.91%  2.0472ms         1  2.0472ms  2.0472ms  2.0472ms  [CUDA memcpy DtoH]

==24432== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 45.57%  220.68ms        51  4.3271ms  4.2833ms  4.3507ms  cudaThreadSynchronize
 38.47%  186.27ms         3  62.089ms  1.1058ms  184.05ms  cudaMalloc
 12.02%  58.221ms         1  58.221ms  58.221ms  58.221ms  cudaDeviceReset
  1.19%  5.7842ms         3  1.9281ms  1.2763ms  2.5058ms  cudaMemcpy
  1.04%  5.0517ms         3  1.6839ms  7.3910us  5.0366ms  cudaDeviceSynchronize
  0.73%  3.5380ms        51  69.371us  44.914us  89.259us  cudaEventSynchronize
  0.22%  1.0532ms       182  5.7860us       0ns  290.23us  cuDeviceGetAttribute
  0.20%  946.89us        52  18.209us  16.203us  56.285us  cudaLaunch
  0.17%  820.96us       102  8.0480us  5.6850us  20.752us  cudaEventRecord
  0.12%  592.69us         1  592.69us  592.69us  592.69us  cudaGetDeviceProperties
  0.11%  511.11us         3  170.37us  140.71us  186.76us  cudaFree
  0.10%  491.78us        51  9.6420us  9.0960us  10.803us  cudaEventElapsedTime
  0.03%  158.91us         2  79.452us  69.077us  89.828us  cuDeviceGetName
  0.01%  57.424us       260     220ns       0ns  1.1370us  cudaSetupArgument
  0.01%  26.151us        52     502ns     284ns  3.4110us  cudaConfigureCall
  0.00%  17.339us         2  8.6690us  5.6850us  11.654us  cuDeviceTotalMem
  0.00%  12.508us         1  12.508us  12.508us  12.508us  cudaSetDevice
  0.00%  7.3910us         2  3.6950us  1.1370us  6.2540us  cudaEventCreate
  0.00%  2.8420us         3     947ns     284ns  2.2740us  cuDeviceGetCount
  0.00%  2.5590us         6     426ns     284ns     569ns  cuDeviceGet
  0.00%  2.2740us         1  2.2740us  2.2740us  2.2740us  cudaGetDevice
