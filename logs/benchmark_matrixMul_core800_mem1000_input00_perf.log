"arg:-wA=1024 -hA=1024 -wB=1024 -hB=1024" 
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
Adjust Iters to 1976 for meeting time requirement 10 secs.
iterated 1976, average time is 5.072848 msec
Performance= 423.33 GFlop/s, Time= 5.073 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
iterated 50, average time is 5.090413 msec
Performance= 421.87 GFlop/s, Time= 5.090 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
==41044== NVPROF is profiling process 41044, command: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==41044== Warning: Unified Memory Profiling is not supported on the current configuration because a pair of devices without peer-to-peer support is detected on this multi-GPU setup. When peer mappings are not available, system falls back to using zero-copy memory. It can cause kernels, which access unified memory, to run slower. More details can be found at: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-managed-memory
==41044== Profiling application: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==41044== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 98.09%  260.70ms        52  5.0135ms  4.9996ms  5.0268ms  void matrixMulCUDA<int=32>(float*, float*, float*, int, int)
  1.23%  3.2709ms         2  1.6355ms  1.5940ms  1.6769ms  [CUDA memcpy HtoD]
  0.68%  1.7960ms         1  1.7960ms  1.7960ms  1.7960ms  [CUDA memcpy DtoH]

==41044== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 50.69%  259.55ms        51  5.0893ms  5.0645ms  5.1097ms  cudaThreadSynchronize
 34.25%  175.36ms         3  58.453ms  1.1183ms  173.12ms  cudaMalloc
 11.28%  57.762ms         1  57.762ms  57.762ms  57.762ms  cudaDeviceReset
  1.14%  5.8141ms         3  1.9380ms  7.3910us  5.7896ms  cudaDeviceSynchronize
  1.07%  5.4548ms         3  1.8183ms  1.2653ms  2.2136ms  cudaMemcpy
  0.62%  3.1954ms        51  62.655us  36.102us  127.92us  cudaEventSynchronize
  0.21%  1.0538ms       182  5.7890us       0ns  297.63us  cuDeviceGetAttribute
  0.18%  945.75us        52  18.187us  14.782us  53.726us  cudaLaunch
  0.15%  769.51us       102  7.5440us  4.8330us  19.614us  cudaEventRecord
  0.12%  608.33us         3  202.78us  175.11us  219.74us  cudaFree
  0.12%  592.69us         1  592.69us  592.69us  592.69us  cudaGetDeviceProperties
  0.09%  441.19us        51  8.6500us  7.6760us  20.183us  cudaEventElapsedTime
  0.03%  169.71us       260     652ns       0ns  11.939us  cudaSetupArgument
  0.03%  159.76us         2  79.878us  69.076us  90.680us  cuDeviceGetName
  0.02%  92.952us        52  1.7870us     284ns  11.654us  cudaConfigureCall
  0.01%  46.051us         1  46.051us  46.051us  46.051us  cudaSetDevice
  0.00%  11.655us         2  5.8270us  5.6860us  5.9690us  cuDeviceTotalMem
  0.00%  7.1060us         2  3.5530us  1.1370us  5.9690us  cudaEventCreate
  0.00%  3.1270us         3  1.0420us     284ns  2.5580us  cuDeviceGetCount
  0.00%  2.8430us         1  2.8430us  2.8430us  2.8430us  cudaGetDevice
  0.00%  1.9890us         6     331ns     284ns     568ns  cuDeviceGet
