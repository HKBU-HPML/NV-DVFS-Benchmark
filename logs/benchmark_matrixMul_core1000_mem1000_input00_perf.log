"arg:-wA=1024 -hA=1024 -wB=1024 -hB=1024" 
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
Adjust Iters to 2422 for meeting time requirement 10 secs.
iterated 2422, average time is 4.177130 msec
Performance= 514.11 GFlop/s, Time= 4.177 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
iterated 50, average time is 4.204101 msec
Performance= 510.81 GFlop/s, Time= 4.204 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
==43260== NVPROF is profiling process 43260, command: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==43260== Warning: Unified Memory Profiling is not supported on the current configuration because a pair of devices without peer-to-peer support is detected on this multi-GPU setup. When peer mappings are not available, system falls back to using zero-copy memory. It can cause kernels, which access unified memory, to run slower. More details can be found at: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-managed-memory
==43260== Profiling application: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==43260== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 97.52%  214.13ms        52  4.1179ms  4.1045ms  4.1326ms  void matrixMulCUDA<int=32>(float*, float*, float*, int, int)
  1.49%  3.2698ms         2  1.6349ms  1.5953ms  1.6745ms  [CUDA memcpy HtoD]
  0.99%  2.1748ms         1  2.1748ms  2.1748ms  2.1748ms  [CUDA memcpy DtoH]

==43260== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 46.05%  214.68ms        51  4.2093ms  4.1736ms  4.2807ms  cudaThreadSynchronize
 37.31%  173.91ms         3  57.970ms  1.1061ms  171.69ms  cudaMalloc
 12.58%  58.639ms         1  58.639ms  58.639ms  58.639ms  cudaDeviceReset
  1.26%  5.8908ms         3  1.9636ms  1.2633ms  2.6522ms  cudaMemcpy
  1.05%  4.9141ms         3  1.6380ms  7.3910us  4.8990ms  cudaDeviceSynchronize
  0.76%  3.5337ms        51  69.288us  43.777us  87.838us  cudaEventSynchronize
  0.22%  1.0464ms       182  5.7490us       0ns  284.83us  cuDeviceGetAttribute
  0.19%  902.26us        52  17.351us  15.918us  55.716us  cudaLaunch
  0.17%  806.45us       102  7.9060us  5.6850us  21.035us  cudaEventRecord
  0.12%  581.04us         1  581.04us  581.04us  581.04us  cudaGetDeviceProperties
  0.11%  509.97us         3  169.99us  141.56us  188.18us  cudaFree
  0.11%  492.63us        51  9.6590us  9.3800us  11.087us  cudaEventElapsedTime
  0.03%  158.34us         2  79.168us  69.361us  88.975us  cuDeviceGetName
  0.01%  52.589us       260     202ns       0ns  1.1370us  cudaSetupArgument
  0.01%  25.867us        52     497ns     284ns  3.1270us  cudaConfigureCall
  0.00%  12.224us         1  12.224us  12.224us  12.224us  cudaSetDevice
  0.00%  11.371us         2  5.6850us  5.4010us  5.9700us  cuDeviceTotalMem
  0.00%  6.8220us         2  3.4110us  1.1370us  5.6850us  cudaEventCreate
  0.00%  3.6950us         3  1.2310us     284ns  2.8430us  cuDeviceGetCount
  0.00%  3.1270us         6     521ns     284ns  1.1370us  cuDeviceGet
  0.00%  2.5580us         1  2.5580us  2.5580us  2.5580us  cudaGetDevice
