"arg:-wA=1024 -hA=1024 -wB=1024 -hB=1024" 
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
Adjust Iters to 1435 for meeting time requirement 10 secs.
iterated 1435, average time is 6.966730 msec
Performance= 308.25 GFlop/s, Time= 6.967 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "GeForce GTX 980" with compute capability 5.2

MatrixA(1024,1024), MatrixB(1024,1024)
Computing result using CUDA Kernel...
done
iterated 50, average time is 6.994385 msec
Performance= 307.03 GFlop/s, Time= 6.994 msec, Size= 2147483648 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
==74636== NVPROF is profiling process 74636, command: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==74636== Warning: Unified Memory Profiling is not supported on the current configuration because a pair of devices without peer-to-peer support is detected on this multi-GPU setup. When peer mappings are not available, system falls back to using zero-copy memory. It can cause kernels, which access unified memory, to run slower. More details can be found at: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-managed-memory
==74636== Profiling application: applications/matrixMul -wA=1024 -hA=1024 -wB=1024 -hB=1024 -device=1 -iters=50
==74636== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 98.56%  359.33ms        52  6.9103ms  6.8782ms  6.9426ms  void matrixMulCUDA<int=32>(float*, float*, float*, int, int)
  0.89%  3.2603ms         2  1.6301ms  1.5844ms  1.6758ms  [CUDA memcpy HtoD]
  0.54%  1.9858ms         1  1.9858ms  1.9858ms  1.9858ms  [CUDA memcpy DtoH]

==74636== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 56.34%  356.64ms        51  6.9930ms  6.9611ms  7.0264ms  cudaThreadSynchronize
 31.39%  198.69ms         3  66.229ms  1.0984ms  196.48ms  cudaMalloc
  8.76%  55.457ms         1  55.457ms  55.457ms  55.457ms  cudaDeviceReset
  1.22%  7.7417ms         3  2.5806ms  8.5280us  7.7138ms  cudaDeviceSynchronize
  0.89%  5.6390ms         3  1.8797ms  1.2533ms  2.4438ms  cudaMemcpy
  0.53%  3.3773ms        51  66.222us  38.091us  148.67us  cudaEventSynchronize
  0.20%  1.2434ms       102  12.190us  5.1170us  17.624us  cudaEventRecord
  0.19%  1.1845ms        52  22.779us  18.477us  59.979us  cudaLaunch
  0.18%  1.1169ms       182  6.1360us       0ns  321.50us  cuDeviceGetAttribute
  0.10%  636.47us         1  636.47us  636.47us  636.47us  cudaGetDeviceProperties
  0.08%  511.39us         3  170.46us  120.24us  224.57us  cudaFree
  0.07%  440.32us        51  8.6330us  7.9590us  10.234us  cudaEventElapsedTime
  0.02%  152.65us         2  76.325us  71.351us  81.300us  cuDeviceGetName
  0.01%  67.658us       260     260ns       0ns  11.086us  cudaSetupArgument
  0.01%  64.531us        52  1.2400us     284ns  11.371us  cudaConfigureCall
  0.00%  13.360us         1  13.360us  13.360us  13.360us  cudaSetDevice
  0.00%  11.656us         2  5.8280us  5.6860us  5.9700us  cuDeviceTotalMem
  0.00%  9.9500us         2  4.9750us     853ns  9.0970us  cudaEventCreate
  0.00%  3.4120us         3  1.1370us     284ns  2.5590us  cuDeviceGetCount
  0.00%  2.5580us         1  2.5580us  2.5580us  2.5580us  cudaGetDevice
  0.00%  2.2750us         6     379ns     284ns     569ns  cuDeviceGet
